{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cada9e",
   "metadata": {},
   "source": [
    "# CSCE 421 :: Machine Learning :: Texas A&M University :: Fall 2021\n",
    "\n",
    "# Homework 7 (HW7)\n",
    "**Name:** Nikita Udodenko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045ba94",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "- **100 points**\n",
    "- **Due Wednesday, Dec 08, 11:59 pm**\n",
    "\n",
    "In this assignment, you'll be coding up the perceptron algorithm from scratch. Refer to the class slides for the weight update rule.  \n",
    "\n",
    "### Instructions\n",
    "- You are **NOT** allowed to use machine learning libraries such as scikit-learn to build a perceptron for this assignment.\n",
    "- You are required to complete the functions defined in the code blocks following each question. Fill out sections of the code marked `\"YOUR CODE HERE\"`.\n",
    "- You're free to add any number of methods within each class.\n",
    "- You may also add any number of additional code blocks that you deem necessary. \n",
    "- Once you've filled out your solutions, submit the notebook on Canvas following the instructions [here](https://people.engr.tamu.edu/guni/csce421/assignments.html).\n",
    "- Do **NOT** forget to type in your name and UIN at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e368b",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de6830",
   "metadata": {},
   "source": [
    "The `ValueIteration` class in `Value_Iteration.py` contains the implementation for the value iteration algorithm. Complete the `train_episode` and `create_greedy_policy` methods.  \n",
    "\n",
    "### Resources\n",
    "Sutton & Barto, Section 4.4, page 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17f737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AbstractSolver(ABC):\n",
    "\n",
    "    def __init__(self, env, gamma):\n",
    "        self.statistics = [0] * len(Statistics)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def init_stats(self):\n",
    "        self.statistics[1:] = [0] * (len(Statistics)-1)\n",
    "\n",
    "    def run_greedy(self):\n",
    "        \"\"\"\n",
    "        Run the greedy policy post learning\n",
    "        \"\"\"\n",
    "        policy = self.create_greedy_policy()\n",
    "        # Reset the environment and pick the first action\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in range(100):\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            self.env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_episode(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __str__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_greedy_policy(self):\n",
    "        pass\n",
    "    \n",
    "class Statistics(Enum):\n",
    "    Episode = 0\n",
    "    Rewards = 1\n",
    "    Steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fd825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "class ValueIteration(AbstractSolver):\n",
    "\n",
    "    def __init__(self, env, gamma):\n",
    "        assert str(env.observation_space).startswith( 'Discrete' ), str(self) + \\\n",
    "                                                                    \" cannot handle non-discrete state spaces\"\n",
    "        assert str(env.action_space).startswith('Discrete'), str(self) + \" cannot handle non-discrete action spaces\"\n",
    "        super().__init__(env, gamma)\n",
    "        self.V = np.zeros(env.nS)\n",
    "\n",
    "    def train_episode(self):\n",
    "        \"\"\"\n",
    "            Run a single episode of the Value Iteration Algorithm.\n",
    "\n",
    "            Use:\n",
    "                self.env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "                    env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "                    env.nS is a number of states in the environment.\n",
    "                    env.nA is a number of actions in the environment.\n",
    "                self.options.gamma: Gamma discount factor.\n",
    "            \"\"\"\n",
    "\n",
    "        # Update each state...\n",
    "        delta = 0\n",
    "        for s in range(self.env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10.\n",
    "\n",
    "            ################################\n",
    "            #   YOUR IMPLEMENTATION HERE   #\n",
    "            ################################\n",
    "            actions = np.zeros(self.env.nA)\n",
    "            for action in range(self.env.nA):\n",
    "                for prob, next_state, reward, done in self.env.P[s][action]:\n",
    "                    actions[action] += prob * (reward + self.gamma * self.V[next_state])\n",
    "            best = np.max(actions)\n",
    "            delta = max(delta, np.abs(best - self.V[s]))\n",
    "            self.V[s] = best\n",
    "\n",
    "        # In DP methods we don't interact with the environment so we will set the reward to be the sum of state values\n",
    "        # and the number of steps to -1 representing an invalid value\n",
    "        self.statistics[Statistics.Rewards.value] = np.sum(self.V)\n",
    "        self.statistics[Statistics.Steps.value] = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Value Iteration\"\n",
    "\n",
    "    def create_greedy_policy(self):\n",
    "        \"\"\"\n",
    "        Creates a greedy policy based on state values.\n",
    "\n",
    "        Use:\n",
    "            self.env.nA: Number of actions in the environment.\n",
    "\n",
    "        Returns:\n",
    "            A function that takes an observation as input and returns a vector\n",
    "            of action probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        def policy_fn(state):\n",
    "            \n",
    "            ################################\n",
    "            #   YOUR IMPLEMENTATION HERE   #\n",
    "            ################################\n",
    "            policy = np.zeros(self.env.nA)\n",
    "            actions = np.zeros(self.env.nA)\n",
    "            for action in range(self.env.nA):\n",
    "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
    "                    actions[action] += prob * (reward + self.gamma * self.V[next_state])\n",
    "            best = np.argmax(actions)\n",
    "            policy[best] = 1.0\n",
    "            return policy\n",
    "\n",
    "        return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c8ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ID: FrozenLake-v1\n",
      "Domain state space is Discrete(16)\n",
      "Domain action space is Discrete(4)\n",
      "\n",
      "Episode 1: Sum of Values: 0.3333333333333333\n",
      "Episode 2: Sum of Values: 0.6633333333333333\n",
      "Episode 3: Sum of Values: 0.9714333333333333\n",
      "Episode 4: Sum of Values: 1.2154593333333332\n",
      "Episode 5: Sum of Values: 1.4058611733333333\n",
      "Episode 6: Sum of Values: 1.556045614233333\n",
      "Episode 7: Sum of Values: 1.675477822500333\n",
      "Episode 8: Sum of Values: 1.7708598873476733\n",
      "Episode 9: Sum of Values: 1.8472291144269428\n",
      "Episode 10: Sum of Values: 1.908441402766785\n",
      "Episode 11: Sum of Values: 1.9575305082853143\n",
      "Episode 12: Sum of Values: 1.9969055084152645\n",
      "Episode 13: Sum of Values: 2.028489105707634\n",
      "Episode 14: Sum of Values: 2.0540151407907112\n",
      "Episode 15: Sum of Values: 2.0749848326636213\n",
      "Episode 16: Sum of Values: 2.0922761171986224\n",
      "Episode 17: Sum of Values: 2.1065694854090173\n",
      "Episode 18: Sum of Values: 2.1184042179402036\n",
      "Episode 19: Sum of Values: 2.128213925214414\n",
      "Episode 20: Sum of Values: 2.136350829146036\n",
      "Episode 21: Sum of Values: 2.143103131840973\n",
      "Episode 22: Sum of Values: 2.1487078694226294\n",
      "Episode 23: Sum of Values: 2.153360693694359\n",
      "Episode 24: Sum of Values: 2.1572234892735596\n",
      "Episode 25: Sum of Values: 2.160430415656771\n",
      "Episode 26: Sum of Values: 2.1630927686935095\n",
      "Episode 27: Sum of Values: 2.1653029338718963\n",
      "Episode 28: Sum of Values: 2.167137625727141\n",
      "Episode 29: Sum of Values: 2.1686605564641126\n",
      "Episode 30: Sum of Values: 2.169924642294596\n",
      "Episode 31: Sum of Values: 2.1709738318565845\n",
      "Episode 32: Sum of Values: 2.171844623665912\n",
      "Episode 33: Sum of Values: 2.1725673265640486\n",
      "Episode 34: Sum of Values: 2.173167107156903\n",
      "Episode 35: Sum of Values: 2.1736648603994424\n",
      "Episode 36: Sum of Values: 2.1740779331976303\n",
      "Episode 37: Sum of Values: 2.1744207257928663\n",
      "Episode 38: Sum of Values: 2.1747051915034787\n",
      "Episode 39: Sum of Values: 2.1749412519361195\n",
      "Episode 40: Sum of Values: 2.175137141908401\n",
      "Episode 41: Sum of Values: 2.175299695936277\n",
      "Episode 42: Sum of Values: 2.1754345861514235\n",
      "Episode 43: Sum of Values: 2.1755465198573103\n",
      "Episode 44: Sum of Values: 2.1756394035524096\n",
      "Episode 45: Sum of Values: 2.175716479099045\n",
      "Episode 46: Sum of Values: 2.175780436758722\n",
      "Episode 47: Sum of Values: 2.1758335090175254\n",
      "Episode 48: Sum of Values: 2.1758775484617097\n",
      "Episode 49: Sum of Values: 2.1759140924117677\n",
      "Episode 50: Sum of Values: 2.1759444165643753\n",
      "Episode 51: Sum of Values: 2.1759695795101663\n",
      "Episode 52: Sum of Values: 2.175990459678344\n",
      "Episode 53: Sum of Values: 2.1760077859957843\n",
      "Episode 54: Sum of Values: 2.176022163329568\n",
      "Episode 55: Sum of Values: 2.176034093600242\n",
      "Episode 56: Sum of Values: 2.1760439933022693\n",
      "Episode 57: Sum of Values: 2.176052208042937\n",
      "Episode 58: Sum of Values: 2.176059024607002\n",
      "Episode 59: Sum of Values: 2.176064680968111\n",
      "Episode 60: Sum of Values: 2.1760693745963713\n",
      "Episode 61: Sum of Values: 2.176073269352039\n",
      "Episode 62: Sum of Values: 2.17607650120593\n",
      "Episode 63: Sum of Values: 2.176079182986241\n",
      "Episode 64: Sum of Values: 2.1760814083174593\n",
      "Episode 65: Sum of Values: 2.1760832548888818\n",
      "Episode 66: Sum of Values: 2.1760847871668223\n",
      "Episode 67: Sum of Values: 2.176086058645205\n",
      "Episode 68: Sum of Values: 2.176087113713094\n",
      "Episode 69: Sum of Values: 2.1760879892043765\n",
      "Episode 70: Sum of Values: 2.1760887156836732\n",
      "Episode 71: Sum of Values: 2.1760893185133936\n",
      "Episode 72: Sum of Values: 2.17608981873917\n",
      "Episode 73: Sum of Values: 2.1760902338245867\n",
      "Episode 74: Sum of Values: 2.1760905782608617\n",
      "Episode 75: Sum of Values: 2.176090864072751\n",
      "Episode 76: Sum of Values: 2.1760911012383506\n",
      "Episode 77: Sum of Values: 2.1760912980374476\n",
      "Episode 78: Sum of Values: 2.176091461340577\n",
      "Episode 79: Sum of Values: 2.176091596848881\n",
      "Episode 80: Sum of Values: 2.1760917092931473\n",
      "Episode 81: Sum of Values: 2.1760918025989637\n",
      "Episode 82: Sum of Values: 2.176091880023769\n",
      "Episode 83: Sum of Values: 2.1760919442705724\n",
      "Episode 84: Sum of Values: 2.176091997582321\n",
      "Episode 85: Sum of Values: 2.176092041820203\n",
      "Episode 86: Sum of Values: 2.176092078528627\n",
      "Episode 87: Sum of Values: 2.1760921089891347\n",
      "Episode 88: Sum of Values: 2.176092134265146\n",
      "Episode 89: Sum of Values: 2.176092155239082\n",
      "Episode 90: Sum of Values: 2.1760921726431732\n",
      "Episode 91: Sum of Values: 2.1760921870850205\n",
      "Episode 92: Sum of Values: 2.176092199068809\n",
      "Episode 93: Sum of Values: 2.176092209012911\n",
      "Episode 94: Sum of Values: 2.176092217264488\n",
      "Episode 95: Sum of Values: 2.1760922241116147\n",
      "Episode 96: Sum of Values: 2.1760922297933334\n",
      "Episode 97: Sum of Values: 2.176092234508002\n",
      "Episode 98: Sum of Values: 2.1760922384202157\n",
      "Episode 99: Sum of Values: 2.176092241666556\n",
      "Episode 100: Sum of Values: 2.1760922443603556\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "ENV_NAME = 'FrozenLake-v1'\n",
    "SEED = 0\n",
    "MAX_EPISODES = 100\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.1\n",
    "\n",
    "random.seed(0)\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "print(\"Environment ID: {}\".format(ENV_NAME))\n",
    "print(\"Domain state space is {}\".format(env.observation_space))\n",
    "print(\"Domain action space is {}\\n\".format(env.action_space))\n",
    "\n",
    "solver = ValueIteration(env, gamma=GAMMA)\n",
    "\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    solver.init_stats()\n",
    "    solver.statistics[Statistics.Episode.value] += 1\n",
    "    env.reset()\n",
    "    solver.train_episode()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"Episode {}: Sum of Values: {}\".format(i_episode+1,solver.statistics[Statistics.Rewards.value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a60dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "solver.run_greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fc05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
